{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{http://www.mediawiki.org/xml/export-0.10/}siteinfo\n",
      "{http://www.mediawiki.org/xml/export-0.10/}sitename\n",
      "{http://www.mediawiki.org/xml/export-0.10/}dbname\n",
      "{http://www.mediawiki.org/xml/export-0.10/}base\n",
      "{http://www.mediawiki.org/xml/export-0.10/}generator\n",
      "{http://www.mediawiki.org/xml/export-0.10/}case\n",
      "{http://www.mediawiki.org/xml/export-0.10/}namespaces\n",
      "{http://www.mediawiki.org/xml/export-0.10/}namespace\n",
      "{http://www.mediawiki.org/xml/export-0.10/}namespace\n",
      "{http://www.mediawiki.org/xml/export-0.10/}namespace\n",
      "{http://www.mediawiki.org/xml/export-0.10/}namespace\n",
      "{http://www.mediawiki.org/xml/export-0.10/}namespace\n",
      "{http://www.mediawiki.org/xml/export-0.10/}namespace\n",
      "{http://www.mediawiki.org/xml/export-0.10/}namespace\n",
      "{http://www.mediawiki.org/xml/export-0.10/}namespace\n",
      "{http://www.mediawiki.org/xml/export-0.10/}namespace\n",
      "{http://www.mediawiki.org/xml/export-0.10/}namespace\n",
      "{http://www.mediawiki.org/xml/export-0.10/}namespace\n",
      "{http://www.mediawiki.org/xml/export-0.10/}namespace\n",
      "{http://www.mediawiki.org/xml/export-0.10/}namespace\n",
      "{http://www.mediawiki.org/xml/export-0.10/}namespace\n",
      "{http://www.mediawiki.org/xml/export-0.10/}namespace\n",
      "{http://www.mediawiki.org/xml/export-0.10/}namespace\n",
      "{http://www.mediawiki.org/xml/export-0.10/}namespace\n",
      "{http://www.mediawiki.org/xml/export-0.10/}namespace\n",
      "{http://www.mediawiki.org/xml/export-0.10/}namespace\n",
      "{http://www.mediawiki.org/xml/export-0.10/}namespace\n",
      "{http://www.mediawiki.org/xml/export-0.10/}namespace\n",
      "{http://www.mediawiki.org/xml/export-0.10/}namespace\n",
      "{http://www.mediawiki.org/xml/export-0.10/}namespace\n",
      "{http://www.mediawiki.org/xml/export-0.10/}namespace\n",
      "{http://www.mediawiki.org/xml/export-0.10/}namespace\n",
      "{http://www.mediawiki.org/xml/export-0.10/}namespace\n",
      "{http://www.mediawiki.org/xml/export-0.10/}namespace\n",
      "{http://www.mediawiki.org/xml/export-0.10/}namespace\n",
      "{http://www.mediawiki.org/xml/export-0.10/}page\n",
      "{http://www.mediawiki.org/xml/export-0.10/}title\n",
      "{http://www.mediawiki.org/xml/export-0.10/}ns\n",
      "{http://www.mediawiki.org/xml/export-0.10/}id\n",
      "{http://www.mediawiki.org/xml/export-0.10/}redirect\n",
      "{http://www.mediawiki.org/xml/export-0.10/}revision\n",
      "{http://www.mediawiki.org/xml/export-0.10/}id\n",
      "{http://www.mediawiki.org/xml/export-0.10/}parentid\n",
      "{http://www.mediawiki.org/xml/export-0.10/}timestamp\n",
      "{http://www.mediawiki.org/xml/export-0.10/}contributor\n",
      "{http://www.mediawiki.org/xml/export-0.10/}username\n",
      "{http://www.mediawiki.org/xml/export-0.10/}id\n",
      "{http://www.mediawiki.org/xml/export-0.10/}minor\n",
      "{http://www.mediawiki.org/xml/export-0.10/}model\n",
      "{http://www.mediawiki.org/xml/export-0.10/}format\n",
      "{http://www.mediawiki.org/xml/export-0.10/}text\n",
      "{http://www.mediawiki.org/xml/export-0.10/}sha1\n",
      "{http://www.mediawiki.org/xml/export-0.10/}page\n",
      "{http://www.mediawiki.org/xml/export-0.10/}title\n",
      "{http://www.mediawiki.org/xml/export-0.10/}ns\n",
      "{http://www.mediawiki.org/xml/export-0.10/}id\n",
      "{http://www.mediawiki.org/xml/export-0.10/}revision\n",
      "{http://www.mediawiki.org/xml/export-0.10/}id\n",
      "{http://www.mediawiki.org/xml/export-0.10/}parentid\n",
      "{http://www.mediawiki.org/xml/export-0.10/}timestamp\n",
      "{http://www.mediawiki.org/xml/export-0.10/}contributor\n",
      "{http://www.mediawiki.org/xml/export-0.10/}username\n",
      "{http://www.mediawiki.org/xml/export-0.10/}id\n",
      "{http://www.mediawiki.org/xml/export-0.10/}minor\n",
      "{http://www.mediawiki.org/xml/export-0.10/}comment\n",
      "{http://www.mediawiki.org/xml/export-0.10/}model\n",
      "{http://www.mediawiki.org/xml/export-0.10/}format\n",
      "{http://www.mediawiki.org/xml/export-0.10/}text\n",
      "{http://www.mediawiki.org/xml/export-0.10/}sha1\n",
      "{http://www.mediawiki.org/xml/export-0.10/}page\n",
      "{http://www.mediawiki.org/xml/export-0.10/}title\n",
      "{http://www.mediawiki.org/xml/export-0.10/}ns\n",
      "{http://www.mediawiki.org/xml/export-0.10/}id\n",
      "{http://www.mediawiki.org/xml/export-0.10/}revision\n",
      "{http://www.mediawiki.org/xml/export-0.10/}id\n",
      "{http://www.mediawiki.org/xml/export-0.10/}parentid\n",
      "{http://www.mediawiki.org/xml/export-0.10/}timestamp\n",
      "{http://www.mediawiki.org/xml/export-0.10/}contributor\n",
      "{http://www.mediawiki.org/xml/export-0.10/}username\n",
      "{http://www.mediawiki.org/xml/export-0.10/}id\n",
      "{http://www.mediawiki.org/xml/export-0.10/}comment\n",
      "{http://www.mediawiki.org/xml/export-0.10/}model\n",
      "{http://www.mediawiki.org/xml/export-0.10/}format\n",
      "{http://www.mediawiki.org/xml/export-0.10/}text\n",
      "{http://www.mediawiki.org/xml/export-0.10/}sha1\n",
      "{http://www.mediawiki.org/xml/export-0.10/}page\n",
      "{http://www.mediawiki.org/xml/export-0.10/}title\n",
      "{http://www.mediawiki.org/xml/export-0.10/}ns\n",
      "{http://www.mediawiki.org/xml/export-0.10/}id\n",
      "{http://www.mediawiki.org/xml/export-0.10/}redirect\n",
      "{http://www.mediawiki.org/xml/export-0.10/}revision\n",
      "{http://www.mediawiki.org/xml/export-0.10/}id\n",
      "{http://www.mediawiki.org/xml/export-0.10/}parentid\n",
      "{http://www.mediawiki.org/xml/export-0.10/}timestamp\n",
      "{http://www.mediawiki.org/xml/export-0.10/}contributor\n",
      "{http://www.mediawiki.org/xml/export-0.10/}username\n",
      "{http://www.mediawiki.org/xml/export-0.10/}id\n",
      "{http://www.mediawiki.org/xml/export-0.10/}comment\n",
      "{http://www.mediawiki.org/xml/export-0.10/}model\n",
      "{http://www.mediawiki.org/xml/export-0.10/}format\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def view_partial_xml_structure(xml_file, max_elements=20):\n",
    "    \"\"\"\n",
    "    Function to view part of the XML structure without loading the entire file.\n",
    "    \n",
    "    Args:\n",
    "        xml_file: Path to the XML file.\n",
    "        max_elements: Maximum number of elements to display.\n",
    "    \"\"\"\n",
    "    context = ET.iterparse(xml_file, events=('start',))\n",
    "    _, root = next(context)  # Get the root element\n",
    "\n",
    "    elements_displayed = 0\n",
    "    for event, elem in context:\n",
    "        # Print the tag of the element\n",
    "        print(elem.tag)\n",
    "\n",
    "        elements_displayed += 1\n",
    "        if elements_displayed >= max_elements:\n",
    "            break\n",
    "\n",
    "    root.clear()  # Clear the root to release memory\n",
    "\n",
    "# Example usage\n",
    "xml_file_path = \"data/viwiki-20240201-multistream.xml\"\n",
    "view_partial_xml_structure(xml_file_path, max_elements=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import codecs\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "PATH_WIKI_XML = 'data/'\n",
    "FILENAME_WIKI = 'viwiki-20240201-multistream.xml'\n",
    "FILENAME_ARTICLES = 'articles.csv'\n",
    "FILENAME_REDIRECT = 'articles_redirect.csv'\n",
    "FILENAME_TEMPLATE = 'articles_template.csv'\n",
    "ENCODING = \"utf-8\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "def strip_tag_name(t):\n",
    "    t = elem.tag\n",
    "    idx = k = t.rfind(\"}\")\n",
    "    if idx != -1:\n",
    "        t = t[idx + 1:]\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathWikiXML = os.path.join(PATH_WIKI_XML, FILENAME_WIKI)\n",
    "pathArticles = os.path.join(PATH_WIKI_XML, FILENAME_ARTICLES)\n",
    "pathArticlesRedirect = os.path.join(PATH_WIKI_XML, FILENAME_REDIRECT)\n",
    "pathTemplateRedirect = os.path.join(PATH_WIKI_XML, FILENAME_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if files exist, if not, create them\n",
    "if not os.path.exists(pathArticles):\n",
    "    with open(pathArticles, 'w', newline='', encoding=ENCODING) as f:\n",
    "        csv.writer(f).writerow(['id', 'title', 'redirect', 'text'])\n",
    "\n",
    "if not os.path.exists(pathArticlesRedirect):\n",
    "    with open(pathArticlesRedirect, 'w', newline='', encoding=ENCODING) as f:\n",
    "        csv.writer(f).writerow(['id', 'title', 'redirect','text'])\n",
    "\n",
    "if not os.path.exists(pathTemplateRedirect):\n",
    "    with open(pathTemplateRedirect, 'w', newline='', encoding=ENCODING) as f:\n",
    "        csv.writer(f).writerow(['id', 'title','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(text):\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    if '{|' in text or '{{' in text:\n",
    "        inside_basket = False\n",
    "        lines = []\n",
    "        for line in text.split('\\n'):\n",
    "            if not inside_basket and ('{|' in line or '{{' in line):\n",
    "                inside_basket = True\n",
    "                continue\n",
    "            if inside_basket and ('|}' in line or '}}' in line):\n",
    "                inside_basket = False\n",
    "                continue\n",
    "            if line.strip().startswith('|'):\n",
    "                continue\n",
    "            if not inside_basket:\n",
    "                lines.append(line)\n",
    "        return ' '.join(lines).strip()\n",
    "    else:\n",
    "        # If no baskets, extract until first '==' or empty text\n",
    "        return text.split('==', 1)[0].strip()\n",
    "\n",
    "# def extract_text(text):\n",
    "#     if text is None:\n",
    "#         return 'NONE'\n",
    "\n",
    "#     # Pattern to match {{Infobox ... }} or {|{{Infobox ... }} |}\n",
    "#     infobox_pattern = r'\\{\\{\\s*Infobox\\s.*?\\}\\}|\\{\\|\\s*\\{\\{\\s*Infobox\\s.*?\\}\\}\\s*\\|\\}'\n",
    "#     # Pattern to match {| and |}\n",
    "#     table_pattern = r'\\{\\|\\s*|\\s*\\|\\}'\n",
    "    \n",
    "#     end_pattern = r'=='\n",
    "    \n",
    "#     cleaned_text = re.sub(infobox_pattern, '', text, flags=re.DOTALL)\n",
    "#     cleaned_text = re.sub(table_pattern, '', cleaned_text, flags=re.DOTALL)\n",
    "    \n",
    "#     end_pos = re.search(end_pattern, cleaned_text, flags=re.IGNORECASE)\n",
    "#     if end_pos:\n",
    "#         cleaned_text = cleaned_text[:end_pos.start()]\n",
    "    \n",
    "#     # Remove quotes\n",
    "#     cleaned_text = cleaned_text.replace('\"', '')\n",
    "    \n",
    "#     return cleaned_text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(text):\n",
    "    if text is None:\n",
    "        return 'NONE'\n",
    "    # Pattern to match {{Infobox ... }} or {|{{Infobox ... }} |}\n",
    "    infobox_pattern = r'\\{\\{\\s*[^{}]*\\}\\}|\\{\\|\\s*\\{\\{\\s*[^{}]*\\}\\}\\s*\\|\\}'\n",
    "    infobox_pattern = r'\\{\\{\\s*Infobox\\s[^{}]*\\}\\}|\\{\\|\\s*\\{\\{\\s*Infobox\\s[^{}]*\\}\\}\\s*\\|\\}'\n",
    "    # Pattern to match {| and |}\n",
    "    table_pattern = r'\\{\\|\\s*|\\s*\\|\\}'\n",
    "    \n",
    "    end_pattern = r'=='\n",
    "    \n",
    "    curly_braces_pattern = r'\\{\\{'\n",
    "    \n",
    "    cleaned_text = re.sub(infobox_pattern, '', text, flags=re.DOTALL)\n",
    "    cleaned_text = re.sub(table_pattern, '', cleaned_text, flags=re.DOTALL)\n",
    "    \n",
    "    \n",
    "    # Find the position of the first occurrence of '==' or '{{'\n",
    "    end_pos = re.search(end_pattern, cleaned_text)\n",
    "    # end_pos1 = re.search(end_pattern, cleaned_text)\n",
    "    # end_pos2 = re.search(curly_braces_pattern, cleaned_text)\n",
    "    \n",
    "    # If '==' or '{{' found, extract the text before it, otherwise return cleaned text\n",
    "    \n",
    "    if end_pos:\n",
    "        cleaned_text = cleaned_text[:end_pos.start()]\n",
    "    \n",
    "    # if end_pos1 and end_pos2:\n",
    "    #     # Extract text until the first occurrence of '==' or '{{'\n",
    "    #     end_pos = min(end_pos1.start(), end_pos2.start())\n",
    "    #     cleaned_text = cleaned_text[:end_pos]\n",
    "    # elif end_pos1:\n",
    "    #     cleaned_text = cleaned_text[:end_pos1.start()]\n",
    "    # elif end_pos2:\n",
    "    #     cleaned_text = cleaned_text[:end_pos2.start()]\n",
    "    \n",
    "    return cleaned_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(text):\n",
    "    result = []\n",
    "    lines = text.split('\\n')\n",
    "    lines_iter = iter(lines)  # Convert lines to an iterator\n",
    "    inside_brackets = False  # to track if we are inside brackets\n",
    "    skip_special = False  # to track if we are skipping special lines\n",
    "    last_special_line = ''  # Keep track of the last line starting with special characters\n",
    "    \n",
    "    for line in lines_iter:  # Now iterate over the lines iterator\n",
    "        line = line.strip()\n",
    "        if line.startswith('{{Infobox') or line.startswith('{|') or line.startswith('{{Thông tin') or line.startswith('{{Chú thích') or line.startwith('{{Hộp thông tin'):\n",
    "            inside_brackets = True\n",
    "        elif line.endswith('}}') or line.endswith('|}'):\n",
    "            inside_brackets = False\n",
    "            skip_special = False  # Reset skip_special when exiting brackets\n",
    "        elif not inside_brackets and line and not line.startswith('=='):\n",
    "            result.append(line)\n",
    "        elif inside_brackets and line.startswith(('|', '!', '#')):\n",
    "            # Skip lines starting with special characters within brackets\n",
    "            while line.startswith(('|', '!', '#')):\n",
    "                line = next(lines_iter, None)  # Use lines_iter\n",
    "                if line is None:\n",
    "                    break\n",
    "                line = line.strip()\n",
    "            if not skip_special:\n",
    "                last_special_line = line\n",
    "                skip_special = True\n",
    "        elif line and not line.startswith('=='):\n",
    "            # Extract non-empty lines until '==' or '}}'\n",
    "            \n",
    "            # if skip_special and not line.startswith(('|', '!', '#')):\n",
    "            #     result.append(last_special_line)\n",
    "            #     skip_special = False\n",
    "            \n",
    "            result.append(line)\n",
    "            if line.endswith('==') or line.endswith('}}'):\n",
    "                break \n",
    "        elif line.startswith('=='):\n",
    "            break\n",
    "    return '\\n'.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100,000\n",
      "200,000\n",
      "300,000\n",
      "400,000\n",
      "500,000\n",
      "600,000\n",
      "700,000\n",
      "800,000\n",
      "900,000\n",
      "1,000,000\n",
      "1,100,000\n",
      "1,200,000\n",
      "1,300,000\n",
      "1,400,000\n",
      "1,500,000\n",
      "1,600,000\n",
      "1,700,000\n",
      "1,800,000\n",
      "1,900,000\n",
      "2,000,000\n",
      "2,100,000\n",
      "Total runtime: 0:06:50.82\n"
     ]
    }
   ],
   "source": [
    "totalCount = 0\n",
    "articleCount = 0\n",
    "redirectCount = 0\n",
    "templateCount = 0\n",
    "title = None\n",
    "start_time = time.time()\n",
    "\n",
    "with codecs.open(pathArticles, \"w\", ENCODING) as articlesFH, \\\n",
    "        codecs.open(pathArticlesRedirect, \"w\", ENCODING) as redirectFH, \\\n",
    "        codecs.open(pathTemplateRedirect, \"w\", ENCODING) as templateFH:\n",
    "    articlesWriter = csv.writer(articlesFH, quoting=csv.QUOTE_MINIMAL)\n",
    "    redirectWriter = csv.writer(redirectFH, quoting=csv.QUOTE_MINIMAL)\n",
    "    templateWriter = csv.writer(templateFH, quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    articlesWriter.writerow(['id', 'title', 'redirect','text'])\n",
    "    redirectWriter.writerow(['id', 'title', 'redirect'])\n",
    "    templateWriter.writerow(['id', 'title'])\n",
    "\n",
    "    for event, elem in ET.iterparse(pathWikiXML, events=('start', 'end')):\n",
    "        tname = strip_tag_name(elem.tag)\n",
    "\n",
    "        if event == 'start':\n",
    "            if tname == 'page':\n",
    "                title = ''\n",
    "                id = -1\n",
    "                redirect = ''\n",
    "                text = ''\n",
    "                inrevision = False\n",
    "                ns = 0\n",
    "            elif tname == 'revision':\n",
    "                # Do not pick up on revision id's\n",
    "                inrevision = True\n",
    "            elif tname == 'title':\n",
    "                title = elem.text\n",
    "\n",
    "            elif tname == 'id' and not inrevision and elem.text is not None:\n",
    "                id = int(elem.text)\n",
    "            elif tname == 'redirect':\n",
    "                redirect = elem.get('title', '')\n",
    "            elif tname == 'ns' and elem.text is not None:\n",
    "                ns = int(elem.text)\n",
    "            elif tname == 'text' and inrevision:\n",
    "                # text = elem.text if elem.text is not None else None\n",
    "                text = elem.text\n",
    "                if text is not None:\n",
    "                    text = text.strip()\n",
    "                    text = extract_text(text)\n",
    "\n",
    "        elif tname == 'page':\n",
    "            totalCount += 1\n",
    "\n",
    "            if ns == 10:\n",
    "                templateCount += 1\n",
    "                templateWriter.writerow([id, title])\n",
    "            elif len(redirect) > 0:\n",
    "                articleCount += 1\n",
    "                redirectWriter.writerow([id, title, redirect])                \n",
    "            else:\n",
    "                redirectCount += 1\n",
    "                articlesWriter.writerow([id, title, redirect, text])\n",
    "\n",
    "            if totalCount > 1 and (totalCount % 100000) == 0:\n",
    "                print(\"{:,}\".format(totalCount))\n",
    "\n",
    "        elem.clear()\n",
    "\n",
    "time_took = time.time() - start_time\n",
    "print(f\"Total runtime: {hms_string(time_took)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl from WWIKIPEDIA DIRECTLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def get_soup(id):\n",
    "    url = f'https://vi.wikipedia.org/wiki?curid={id}'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve data for ID {id}. Status code: {response.status_code}\")\n",
    "        return None\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def get_paragraphs(soup):\n",
    "    meta_toc = soup.find('meta', {'property': 'mw:PageProp/toc'})\n",
    "    if meta_toc:\n",
    "        paragraphs = meta_toc.find_all_previous('p')\n",
    "    else:\n",
    "        h2_toc = soup.find('h2')\n",
    "        paragraphs = h2_toc.find_all_previous('p')\n",
    "    paragraphs.reverse()\n",
    "    \n",
    "    all_text = '\\n\\n'.join([paragraph.text.strip() for paragraph in paragraphs])\n",
    "    return all_text\n",
    "\n",
    "def check_person(soup, title):\n",
    "    infobox_table = soup.find('table', {'class': 'infobox'})\n",
    "    if infobox_table is None:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def main(start_id, end_id):\n",
    "    data = []\n",
    "    break_title = ['Wikipedia', 'MediaWiki', 'Bản mẫu', 'Cổng thông tin', 'Thảo luận', 'Thành viên', 'Tập tin', \n",
    "                   'Trợ giúp', 'Thể loại', 'Cổng thông tin', 'TimedText', 'TimedText talk', 'Mô đun', 'Tiện ích', 'Định nghĩa tiện ích']\n",
    "\n",
    "    with open('fix_text.txt', 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip().split(':')\n",
    "            data.append(line)\n",
    "\n",
    "    index = []\n",
    "    title = []\n",
    "    text_list = []\n",
    "    \n",
    "    for i in data:\n",
    "        index.append(i[0])\n",
    "        title.append(i[1])\n",
    "        \n",
    "    # Write to CSV directly\n",
    "    csv_file = 'extracted_data_2.csv'\n",
    "    with open(csv_file, mode='w', encoding='utf-8-sig', newline='') as file:\n",
    "        fieldnames = ['id', 'title', 'text']\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        start_index = index.index(start_id)\n",
    "        end_index = index.index(end_id)\n",
    "\n",
    "        for i in range(start_index, end_index + 1):  # +1 to include the end_id\n",
    "            current_title = title[i]\n",
    "\n",
    "            if \"Thảo luận\" in current_title or current_title in break_title:\n",
    "                print(f\"Skipping {current_title}...\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                soup = get_soup(index[i])\n",
    "                if soup:\n",
    "                    if check_person(soup, current_title):\n",
    "                        text = get_paragraphs(soup)\n",
    "                        if text:\n",
    "                            writer.writerow({'id': index[i], 'title': current_title, 'text': text})\n",
    "                            print(f\"Extracted and wrote data for {current_title}\")\n",
    "                        else:\n",
    "                            print(f\"No content extracted for {current_title}\")\n",
    "                    else:\n",
    "                        print(f\"No infobox found for {current_title}\")\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve data for {current_title}\")\n",
    "            except AttributeError as e:\n",
    "                print(f\"AttributeError: Skipping ID {index[i]}\")\n",
    "                continue\n",
    "\n",
    "    print(\"CSV file saved.\")\n",
    "\n",
    "# Start and end extracting from specific IDs\n",
    "start_id = '844208'\n",
    "end_id = '844210'\n",
    "main(start_id, end_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract id missing text in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def extract_ids_with_empty_text(csv_file, output_file):\n",
    "    with open(csv_file, mode='r') as csvfile, open(output_file, mode='w') as fix_text_file:\n",
    "        csv_reader = csv.DictReader(csvfile)\n",
    "        for row in csv_reader:\n",
    "            if not row['text']:  # Check if 'text' field is empty\n",
    "                # Write 'id' and 'title' with a separator ':' to fix_text.txt\n",
    "                fix_text_file.write(row['id'] + '\\n')\n",
    "\n",
    "    print(\"Extraction complete.\")\n",
    "\n",
    "# Usage\n",
    "csv_file = 'D:/scap_wiki/data/articles_with_image_test_3.csv'\n",
    "output_file = 'tempo.txt'\n",
    "extract_ids_with_empty_text(csv_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers extracted and saved to extracted_numbers.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "with open('temptemp.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "# Use regular expression to find all numbers after \"ID\"\n",
    "numbers = re.findall(r'ID (\\d+)', text)\n",
    "\n",
    "# Write the extracted numbers to a text file\n",
    "with open('extracted_numbers.txt', 'w') as file:\n",
    "    for number in numbers:\n",
    "        file.write(number + '\\n')\n",
    "\n",
    "print(\"Numbers extracted and saved to extracted_numbers.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove fully non text containg articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to 'articles_with_image_test_8.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/articles_with_image_test_7.csv')\n",
    "# Read IDs from a text file\n",
    "with open('compare.txt', 'r') as file:\n",
    "    ids_to_remove = [int(line.strip()) for line in file]\n",
    "# Filter out rows with specified IDs\n",
    "filtered_df = df[~df['id'].isin(ids_to_remove)]\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "filtered_df.to_csv('data/articles_with_image_test_9.csv', index=False)\n",
    "print(\"Filtered data saved to 'articles_with_image_test_8.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374763\n"
     ]
    }
   ],
   "source": [
    "#check length of id\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/articles_with_image_test_7.csv')\n",
    "\n",
    "# Count the number of rows in the DataFrame object using the built-in len() function\n",
    "num_lines = len(df)\n",
    "print(num_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove {{ [[ ''' ]]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters removed and saved to convert_file.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# List of characters to remove\n",
    "chars_to_remove = [\"{{\", \"}}\", \"[[\", \"]]\", \"'''\", \"'''''\"]\n",
    "\n",
    "# Function to remove characters\n",
    "def remove_chars(text):\n",
    "    for char in chars_to_remove:\n",
    "        text = text.replace(char, '')\n",
    "    return text\n",
    "\n",
    "# Input and Output file paths\n",
    "input_file = 'ok_di.csv'\n",
    "output_file = 'convert_file.csv'\n",
    "\n",
    "# Read CSV file, remove characters, and write to new CSV file\n",
    "with open(input_file, mode='r') as infile, open(output_file, mode='w', newline='') as outfile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    fieldnames = reader.fieldnames\n",
    "\n",
    "\n",
    "    # Write header\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in reader:\n",
    "        # Assuming the text column is 'Text'\n",
    "        text_column = row['text']\n",
    "        cleaned_text = remove_chars(text_column)\n",
    "        \n",
    "        # Update the row with cleaned text\n",
    "        row['text'] = cleaned_text\n",
    "\n",
    "        # Write the row to the output CSV file\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"Characters removed and saved to\", output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove the tagging of comment: [1],[2],[3],...\n",
    "\n",
    "remove unknown character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning complete. Cleaned data saved to: new4.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Check if 'text' is a string\n",
    "        # Remove [1], [2], [3], etc.\n",
    "        text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "        # Replace U+00A0 with space\n",
    "        text = text.replace('\\u00A0', ' ')\n",
    "    return text\n",
    "\n",
    "# Input and Output file paths\n",
    "input_file = 'new3.csv'\n",
    "output_file = 'new4.csv'\n",
    "\n",
    "# Chunk size for reading large CSV file\n",
    "chunk_size = 10_000  # Adjust as needed\n",
    "\n",
    "chunk_iterator = pd.read_csv(input_file, chunksize=chunk_size, encoding='utf-8')\n",
    "\n",
    "# Loop through the chunks and save to the output file\n",
    "for i, chunk in enumerate(chunk_iterator):\n",
    "    # Clean the 'text' column\n",
    "    chunk['text'] = chunk['text'].apply(clean_text)\n",
    "    \n",
    "    # Append to output file (create header for the first chunk)\n",
    "    if i == 0:\n",
    "        chunk.to_csv(output_file, mode='w', index=False)\n",
    "    else:\n",
    "        chunk.to_csv(output_file, mode='a', index=False, header=False)\n",
    "\n",
    "print(\"Cleaning complete. Cleaned data saved to:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove dupicate ids ans sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates removed and data sorted by 'id'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('data/articles_with_image.csv')\n",
    "\n",
    "# Remove duplicates based on 'id' column\n",
    "df = df.drop_duplicates(subset=['id'], keep='first')\n",
    "\n",
    "# Sort the DataFrame by 'id'\n",
    "df_sorted = df.sort_values('id')\n",
    "\n",
    "# If you want to overwrite the original file with the sorted and cleaned data:\n",
    "# df_sorted.to_csv('article.csv', index=False)\n",
    "\n",
    "# If you want to save to a new file:\n",
    "df_sorted.to_csv('data/articles_with_image_test_1.csv', index=False)\n",
    "\n",
    "print(\"Duplicates removed and data sorted by 'id'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from image reduce the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Load the pre-trained face detection model\n",
    "face_model = cv2.dnn.readNetFromCaffe(\n",
    "    \"D:/scap_wiki/deploy.prototxt.txt\",\n",
    "    \"D:/scap_wiki/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    ")\n",
    "\n",
    "# Function to check if an image contains a human face\n",
    "def is_human_face(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    (h, w) = image.shape[:2]\n",
    "\n",
    "    # Preprocess the image for face detection\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "\n",
    "    # Pass the blob through the network to detect faces\n",
    "    face_model.setInput(blob)\n",
    "    detections = face_model.forward()\n",
    "\n",
    "    # Check if any faces were detected\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        # Minimum confidence threshold for considering a face detection\n",
    "        if confidence > 0.5:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Path to the folder containing JPG images\n",
    "folder_path = \"data/face_wiki\"\n",
    "\n",
    "# Create a folder to save images with human faces\n",
    "output_folder = \"data/face_wiki_2\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate through images in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Check if the image contains a human face\n",
    "        if is_human_face(image_path):\n",
    "            # If yes, save it to the output folder\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "            cv2.imwrite(output_path, cv2.imread(image_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of image names saved to image_list.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Directory containing the images\n",
    "directory = 'data/face_wiki_2'\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# Filter files with .jpg extension\n",
    "jpg_files = [file for file in files if file.endswith('.jpg')]\n",
    "\n",
    "# Remove the .jpg extension from the file names\n",
    "image_names = [file.split('.')[0] for file in jpg_files]\n",
    "\n",
    "# Name of the text file to save the list\n",
    "txt_file = 'image_list.txt'\n",
    "\n",
    "# Write the list of image names to a text file\n",
    "with open(txt_file, 'w') as f:\n",
    "    for name in image_names:\n",
    "        f.write(name + '\\n')\n",
    "\n",
    "print(f\"List of image names saved to {txt_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Function to read data from CSV file and return it as a dictionary\n",
    "def read_csv(filename):\n",
    "    data = {}\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            data[row['id']] = {'title': row['title'], 'text': row['text']}\n",
    "    return data\n",
    "\n",
    "# Function to match image names with data and write to a new CSV file\n",
    "def match_and_write(image_list_file, data_csv_file, output_csv_file):\n",
    "    # Read data from CSV file\n",
    "    data = read_csv(data_csv_file)\n",
    "    \n",
    "    # Open text file containing image names\n",
    "    with open(image_list_file, 'r') as file:\n",
    "        image_names = file.readlines()\n",
    "        image_names = [name.strip() for name in image_names]  # Remove newline characters\n",
    "    \n",
    "    # Create a new CSV file and write header\n",
    "    with open(output_csv_file, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['id', 'title', 'text']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Match image names with data and write to CSV\n",
    "        for image_name in image_names:\n",
    "            image_id = image_name.split('.')[0]  # Assuming image names are in the format id.jpg\n",
    "            if image_id in data:\n",
    "                writer.writerow({'id': image_id, 'title': data[image_id]['title'], 'text': data[image_id]['text']})\n",
    "\n",
    "# Example usage:\n",
    "image_list_file = 'image_list.txt'\n",
    "data_csv_file = 'data/articles.csv'\n",
    "output_csv_file = 'data/matched_articles.csv'\n",
    "\n",
    "match_and_write(image_list_file, data_csv_file, output_csv_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert to parquet datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to CSV and image folders\n",
    "data_folder = '/scap_wiki/data'\n",
    "working_folder = '/scap_wiki/working'\n",
    "images_folder = '/scap_wiki/data/face_wiki_2'\n",
    "\n",
    "# Read the CSV file\n",
    "csv_path = os.path.join(data_folder, 'matched_articles.csv')\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read and encode image to bytes\n",
    "def read_and_encode_image(image_path):\n",
    "    with open(image_path, 'rb') as f:\n",
    "        image_bytes = f.read()\n",
    "    return {'bytes': image_bytes}  # Return as dictionary with 'bytes' key\n",
    "\n",
    "# Batch size for processing images\n",
    "batch_size = 100\n",
    "num_images = len(df)\n",
    "\n",
    "# Path to the Parquet file\n",
    "parquet_path = os.path.join(working_folder, 'face_wiki.parquet')\n",
    "\n",
    "# Define a custom PyArrow type for 'image' column\n",
    "custom_type = pa.struct([\n",
    "    pa.field(\"bytes\", pa.binary())\n",
    "])\n",
    "\n",
    "# Initialize a Parquet schema with 'image', 'title', and 'text' columns\n",
    "schema = pa.schema([\n",
    "    ('image', custom_type),\n",
    "    ('title', pa.string()),\n",
    "    ('text', pa.string())\n",
    "])\n",
    "\n",
    "# Function to convert image data to PyArrow struct\n",
    "def image_to_struct(image_path):\n",
    "    image_bytes = read_and_encode_image(image_path)['bytes']\n",
    "    return pa.array([(image_bytes,)], type=custom_type)\n",
    "\n",
    "# Function to save batch to Parquet\n",
    "def save_batch_to_parquet(batch_df, parquet_writer):\n",
    "    image_data = [image_to_struct(os.path.join(images_folder, f'{x}.jpg')) for x in batch_df['id']]\n",
    "    batch_df['image'] = pa.chunked_array(image_data)\n",
    "    table = pa.Table.from_pandas(batch_df[['image', 'title', 'text']], schema=schema)\n",
    "    parquet_writer.write_table(table)\n",
    "\n",
    "# Create a Parquet writer\n",
    "with pq.ParquetWriter(parquet_path, schema) as writer:\n",
    "    # Iterate through batches of images\n",
    "    for i in range(0, num_images, batch_size):\n",
    "        if i + batch_size < num_images:\n",
    "            # If not the last batch\n",
    "            batch_df = df.iloc[i:i+batch_size].copy()\n",
    "        else:\n",
    "            # If last batch, adjust the batch size\n",
    "            batch_df = df.iloc[i:].copy()\n",
    "        save_batch_to_parquet(batch_df, writer)\n",
    "\n",
    "print(\"Parquet file saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
